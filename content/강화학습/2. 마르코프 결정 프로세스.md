
## 마르코프 프로세스

**강화학습**은 **순차적 의사결정 문제**를 푸는 **방법론**.

**순차적 의사결정 문제**<sup>Sequential decision making</sup>는 결국 **MDP**<sup>Markov Decision Process</sup> 라는 개념을 통해서 정확하게 표현할 수 있다.
$$MP \equiv (S, P)$$
- 상태의 집합 **S** <br>
	$S= \{s_0, s_1, s_2, s_3, s_4 \}$


- 전이 확률 행렬 **P** <br>
	: 전이 확률<sup>transition probability</sup> 는 상태 s 에서 다음 상태 s' 에 도착할 확률을 가리킨다. <br>
	$$P_{ss'}$$
- $P_{ss'}$ 의 조건부 확률 개념을 이용한 표현 방식 <br>
	$$P_{ss'} = \mathbb{P}[S_{t+1} = s'|S_t = s]$$
	전이 확률은 s 와 s'에 대하여 행렬의 형태로 표현할 수 있다.


### 마르코프 성질
마르코프 프로세스의 모든 상태<sup>state</sup>는 마르코프 성질<sup>Markov Property</sup>를 따른다.

$$\mathbb{P}[s_{t+1}|s_t] = \mathbb{P}[s_{t+1}|s_1, s_2,...,s_t]$$
" 미래는 오로지 현재에 의해 결정된다 "


- **마르코프한 상태** <sup>Markovian state</sup> <br>
	시스템의 다음 상태가 **오직 현재 상태**에 의해서만 결정될 때, 그 상태를 마르코프한 상태라고 합니다. 즉, 과거의 상태는 미래의 상태에 아무런 영향을 미치지 않는다는 의미입니다.
	
	ex) 
	- 체스
	- 바둑
	- **날씨:** 오늘 날씨가 맑으면 내일 비가 올 확률은 오늘 날씨에만 의존하며, 어제 날씨와는 무관합니다.
	- **주식 시장:** 주식 시장의 내일 주가는 오늘의 주가와 시장 상황에 의해 결정되며, 과거의 주가 변동은 직접적인 영향을 미치지 않습니다.
	- **랜덤워크:** 다음 위치는 현재 위치와 이동 방향에만 의존하며, 이전 위치는 고려하지 않습니다.


- 마르코프하지 않은 상태


### 마르코프 리워드 프로세스
마르코프 프로세스에 보상의 개념이 추가되면 **마르코프 리워드 프로세스**<sup>Markov Reward Process</sup> 가 된다.

마르코프 프로세스는 $MP \equiv (S, P)$ 상태의 집합 S, 전이 확률 행렬 P로 정의 되는데, 

MRP를 정의하기 위해서는 보상함수 R과 $\gamma$ 감마 라는 감쇠 인자 2가지 요소가 추가로 필요하다.

$MRP \equiv (S, P, R, \gamma )$ 

- 상태$^{State}$ 의 집합 $S$
- 전이 확률 행렬 $P$
- 보상 함수 $R$
	R은 어떤 생태 s에 도착했을 때 받게 되는 보상을 의미
	$R = \mathbb{E}[R_t|S_t = s]$
- 감쇠 인자  $\gamma$ 
	$\gamma$ 는 0에서 1사이의 숫자이다. 강화 학습에서 미래에 얻을 보상에 비해 당장 얻는 보상을 얼마나 더 중요하게 여길지를 나타내는 파라미터.
	구체적으로 미래에 얻을 보상의 값에 $\gamma$ 가 여러번 곱해지며 그 값을 작게 한다.


### 감쇠된 보상의 합, Return

MRP에서는 MP와 다르게 상태($s$)가 바뀔때마다 해당하는 보상($R$)을 얻는다. (T는 타임스텝)
$s_0, R_0, s_1, R_1, s_2, R_2,..., s_T, R_T$

이와 같은 하나의 여정을 강화 학습에서는 에피소드<sup>episode</sup> 라 한다.

**리턴**<sup>return</sup> $G_t$ 을 정의하면,

$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...$


### $\gamma$ 는 왜 필요한가?
$\gamma$는 

- 수학적 편리성
- 사람의 선호 반영
- 미래에 대한 불확실성 반영


### 에피소드 샘플링


### 상태 가치 함수
State Value Function
