---
title: 2. 마르코프 결정 프로세스
aliases:
  - MDP
---
## 마르코프 프로세스

강화학습은 "순차적 의사결정 문제"를 푸는 "방법론".

"순차적 의사결정<sup>Sequential decision making</sup> 문제"는 결국 **MDP**<sup>Markov Decision Process</sup> 라는 개념을 통해서 정확하게 표현할 수 있다.

<div style="text-align: center;"
><img src ="https://velog.velcdn.com/images/1ncarnati0n/post/4d81d8f0-a5d1-4b30-867b-8749bcb78463/image.png" width = "460" /> 
<p>| 아이가 잠이드는 Markov Process 그림 |</p>
</div>

$$ 
MP \equiv (S, P)
$$
**상태의 집합** $S$ 

$$
S= \{s_0, s_1, s_2, s_3, s_4 \}
$$


**전이 확률 행렬** $P$ 
전이 확률<sup>transition probability</sup> 은 상태 $s$ 에서 다음 상태 $s'$ 에 도착할 확률을 가리킨다.	
$$
P_{ss'}
$$
$P_{ss'}$ 의 조건부 확률 개념을 이용한 표현 방식
$$
P_{ss'} = \mathbb{P}[S_{t+1} = s'|S_t = s]
$$
전이 확률은 $s$ 와 $s'$ 에 대하여 행렬의 형태로 표현할 수 있다. 위 그림을 예로 상태 s 5개를 행과 열로 하여
확률 값을 매트릭스로 표현한다.

$P=$

|             |           |            |            |             |            |
| ----------- | --------- | ---------- | ---------- | ----------- | ---------- |
|             | 누움($s_0$) | 일어남($s_1$) | 눈감음($s_2$) | 잠이 옴($s_3$) | 잠 듦($s_4$) |
| 누움($s_0$)   |           | 0.4        | 0.6        |             |            |
| 일어남($s_1$)  | 0.1       | 0.9        |            |             |            |
| 눈감음($s_2$)  |           |            |            | 0.7         | 0.3        |
| 잠이 옴($s_3$) |           |            |            |             | 1.0        |
| 잠 듦($s_4$)  |           |            |            |             | 1.0        |

<br>

### 마르코프 성질
마르코프 프로세스의 모든 상태<sup>state</sup>는 ==마르코프 성질==<sup>Markov Property</sup>를 따른다.

$$
\mathbb{P}[s_{t+1}|s_t] = \mathbb{P}[s_{t+1}|s_1, s_2,...,s_t]
$$
<div style="text-align: center;, font-weight: bold;, font-size: 9px;, color: #333; "> 미래는 오로지 현재에 의해 결정된다 " </div>


- **마르코프한 상태** <sup>Markovian state</sup> <br>
	시스템의 다음 상태가 **오직 현재 상태**에 의해서만 결정될 때, 그 상태를 마르코프한 상태라고 합니다. 즉, 과거의 상태는 미래의 상태에 아무런 영향을 미치지 않는다는 의미입니다.
	
	ex) 
	- 체스
	- 바둑
	- **날씨:** 오늘 날씨가 맑으면 내일 비가 올 확률은 오늘 날씨에만 의존하며, 어제 날씨와는 무관합니다.
	- **주식 시장:** 주식 시장의 내일 주가는 오늘의 주가와 시장 상황에 의해 결정되며, 과거의 주가 변동은 직접적인 영향을 미치지 않습니다.
	- **랜덤워크:** 다음 위치는 현재 위치와 이동 방향에만 의존하며, 이전 위치는 고려하지 않습니다.


- 마르코프하지 않은 상태
  


### 마르코프 리워드 프로세스
마르코프 프로세스에 보상의 개념이 추가되면 **마르코프 리워드 프로세스**<sup>Markov Reward Process</sup> 가 된다.

마르코프 프로세스는 $MP \equiv (S, P)$ 상태의 집합 $S$, 전이 확률 행렬 $P$ 로 정의 되는데, 

$MRP$를 정의하기 위해서는 보상함수 $R$ 과 $\gamma$ 감마 라는 감쇠 인자 "2가지 요소"가 추가로 필요하다.

$$
MRP \equiv (S, P, R, \gamma )
$$
- 상태$^{State}$ 의 집합 $S$
- 전이 확률 행렬 $P$
- 보상 함수 $R$
	$R$ 은 어떤 생태 $s$ 에 도착했을 때 받게 되는 보상을 의미
$$
R = \mathbb{E}[R_t|S_t = s]
$$
- 감쇠 인자  $\gamma$ 
	$\gamma$ 는 0에서 1사이의 숫자이다. 강화 학습에서 미래에 얻을 보상에 비해 당장 얻는 보상을 얼마나 더 중요하게 여길지를 나타내는 파라미터.
	구체적으로 미래에 얻을 보상의 값에 $\gamma$ 가 여러번 곱해지며 그 값을 작게 한다.


### 감쇠된 보상의 합, Return
$MRP$ 에서는 $MP$ 와 다르게 상태($s$)가 바뀔때마다 해당하는 보상($R$)을 얻는다. ($T$는 타임스텝)
$$
s_0, R_0, s_1, R_1, s_2, R_2,..., s_T, R_T
$$
이와 같은 하나의 여정을 강화 학습에서는 에피소드<sup>episode</sup> 라 한다.

**리턴**<sup>return</sup> $G_t$ 을 정의하면,
$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...
$$

### $\gamma$ 는 왜 필요한가?
$\gamma$는 

- 수학적 편리성
- 사람의 선호 반영
- 미래에 대한 불확실성 반영


### 에피소드 샘플링


### 상태 가치 함수
State Value Function
